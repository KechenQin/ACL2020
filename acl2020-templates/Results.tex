\section{Results and Analysis}

\subsection{Experimental Setup}
We conduct experiments on 3 multi-hop KBQA datasets, \textsc{WebQuestionSP} (WQSP) \cite{DBLP:conf/acl/YihCHG15}, \textsc{ComplexWebQuestion}-1.1 (CWQ) \cite{DBLP:journals/corr/abs-1807-09623}, and \textsc{PathQuestion-Large} (PQL) \cite{DBLP:conf/coling/ZhouHZ18}, and use the original train/dev/test split. The original PQL release contains two subsets: PQL2H and PQL3H, which contains only 2-hop and 3-hop questions correspondingly. \newcite{DBLP:conf/naacl/ChenCCNK19} then combined these two subsets and renamed the unified dataset as PQL+. Table \ref{tab:stats} contains statistics of these datasets. For questions with multiple answers, we use each answer to construct a question-answer (QA) pair. All of the three datasets use Freebase \cite{freebase:datadumps} as the supporting knowledge base. For WQSP and CWQ, we build a subgraph in a similar way as in \cite{DBLP:conf/emnlp/SunDZMSC18}, in order to generate the entity and relation candidates. For PQL, the original paper provides a subgraph of the Freebase. 

\begin{table}[h]\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
& \#train & \#valid & \#test & max\_hops & path \textgreater 1\\
\hline
WQSP  & 2677    & 297     & 1639   & 2  & 79.4\%      \\
CWQ   & 27639        &    3519    &   3531     &   6   &  83.4\%  \\
PQL2H & 1275    & 159     & 160    & 2  & 12.5\%      \\
PQL3H & 1649    & 206     & 207    & 3  &  45.2\%  \\
PQL+  & 2924    & 365     & 367    & 3  & 30.6\%      \\
\hline
\end{tabular}
}
\caption{\fontsize{10}{12}\selectfont Statistics of the datasets. %Note that CWQ dataset does not come with relation path annotation. We get max\_hops by counting the number of relations used in the SPARQL language.
}\label{tab:stats}
\end{table}

We implement our model using \textsc{tensorflow-1.11.0} and choose S-MART \cite{DBLP:journals/corr/YangC16a} and AllenNLP \cite{Gardner2017AllenNLP} as our entity linking tools\footnote{Entity linking results on \textsc{WebQuestionSP} can be found at \url{https://github.com/scottyih/STAGG}}. We test three different graph embedding methods \textsc{Word2vec} \cite{DBLP:journals/corr/abs-1301-3781}, \textsc{TransE} \cite{DBLP:conf/nips/BordesUGWY13}, and \textsc{HolE} \cite{DBLP:journals/corr/TrouillonN17}, and decide to use \textsc{TransE} in our final experiment based on validation performance. The best tuned parameters on development set are summarized as follows: the dynamic function for RNNs is chosen as a gated recurrent units (GRU) with 2 layers and at most 30 units in decoder. The size of the GRU unit and all other embedding layers are set as 300. The threshold $k_1$ is set to be: 15 plus the number of answers in the ground truth answer set, and $k_2$ is top 50\%. We set the dropout rate as 0.3, and train the model using an Adam optimizer with a learning rate of $0.0005$. The Beam size is set to be 12 for both training and inference. We adopt the average F1 score and the set accuracy as our main evaluation metrics. %and compare our method with the following methods: 
It is worth noticing that: except our methods' results, all other experimental results are obtained from early published papers. Details of these models can be found from our referenced papers.

%We test different objective functions on WQSP and CWQ datasets. For WQSP, the dataset is originally labeled with multiple relation paths for each QA pair. We thus use them to train our model with joint probability objective. For CWQ, which does not come with any labeled relation path, we parse the given SPARQL queries into relation paths as ground truth labels in single path experiment. The multiple paths experiment is done by using both DFS and trained single path model to extract extra relation paths which points to the answer.


\subsection{Experimental Results}

\subsubsection{WebQuestionSP}

\textsc{WebQuestionSP} is a dataset that has been widely used for relation extraction and end-to-end KBQA task. It contains 1 or 2 hops questions with ground truth relation path labels. In Table \ref{tab:wqsp_cwq} we compare our method to state-of-the-art models. All comparisons are divided into two groups based on different training supervision. The first block shows methods that only trained with final answer as the supervision, and the second block contains methods that using different additional annotations, such as the description of the entities, relation path annotations, or parsing results of the query. Experimental results that our model performs better than all other methods except NSM \cite{DBLP:conf/acl/LiangBLFL17}. Although NSM only relies on answer to train their model, it requires many prior knowledges, such as a big vocabulary to train word and graph embeddings, type of the graph property, and pre-defined templates of generating program. The experiments from their papers show that these knowledge play a very important role in the system, and their F1 score drops from 69.0 to 60.7 by not using the pretrained embeddings. In contrast, our model supports a training method that takes only raw QA pairs and the facts in knowledge base, and does not rely on any additional labels and pre-defined knowledge. Also, NSM is only tested on a single dataset, \emph{i.e.} WQSP. It is unclear whether they could perform consistently well on different datasets. Among all the methods, STAGG perform the best when additional annotation is provided, and we can see a clear drop between \textsc{STAGG\_SP} and \textsc{STAGG\_answer} when there is no such annotation available.

%By comparing performance of using different objectives as shown in the second block of the table, we can see that there is a significant improvement by considering multiple relation paths in training. The performance gap between joint objective and marginal objective demonstrates that our proposed marginal objective is a much better way to train a model with multiple relation paths. We do not observe a very different results by using or not using labeled relation paths, which is a good signal. 


\begin{table}[h]\centering
\resizebox{1.01\columnwidth}{!}{
\begin{tabular}{|l|c|c|}
\hline
                           & WQSP & CWQ \\
\hline
KV-MemNN* \cite{DBLP:conf/emnlp/MillerFDKBW16}    &  38.6 &  -      \\
STAGG\_answer* \cite{DBLP:conf/acl/YihRMCS16}  &  66.8 &  -      \\
NSM* \cite{DBLP:conf/acl/LiangBLFL17}  &  69.0 &  -      \\
GRAFT-Net* \cite{DBLP:conf/emnlp/SunDZMSC18}                &  62.8 &  26.0      \\
\hline
STAGG\_SP \cite{DBLP:conf/acl/YihRMCS16}  &  \textbf{71.7} &  -      \\
HR-BiLSTM \cite{DBLP:conf/acl/YuYHSXZ17}                  & 62.3 & 31.2         \\
KBQA-GST \cite{DBLP:conf/ijcai/LanW019}          &  67.9 &  36.5      \\
\hline
%Our Method-joint\_prob             &     62.1  &   38.0        \\
%Our Method-joint\_prob\_short             &     58.4  &           \\
%Our Method-joint\_prob\_random             &     58.8  &           \\
%Our Method-joint\_prob\_multiple\_paths             &  63.9     &     -       \\
%Our Method-marginal\_prob\_with\_true\_label             &    \textbf{68.5}    &       34.8     \\
Our Method-marginal\_prob*             &   67.9    &      \textbf{38.4}    \\
%Our Method-obj3+new\_decode  &     &          \\
\hline
\end{tabular}
}
\caption{\fontsize{10}{12}\selectfont We report F1 on WQSP and CWQ. $*$ means this method only requires the final answer as the annotation, and other methods need annotated relation paths or some other annotations.}\label{tab:wqsp_cwq}
\end{table}

\begin{table}[h]\centering
\resizebox{1.01\columnwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
                               & \multicolumn{3}{c|}{WQSP}     & \multicolumn{3}{c|}{CWQ}     \\ \hline
                               & 1 path & \textgreater{}1 path & all  & 1 path & \textgreater 1 path & all \\ \hline
Our Method-joint\_prob         & 60.8   & 63.3      & 62.1 &           &    & 38.0               \\ 
Our Method-joint\_prob\_short  & 60.0    & 57.0        &58.4&              &        &                     \\ 
Our Method-joint\_prob\_random &  59.7      &  58.1       &58.8&             &        &                     \\ 
Our Method-joint\_prob\_multiple\_path &    63.1    &   64.2      &63.7&             & -       &                     \\ 
Our Method-marginal\_prob      & 66.0   & 69.3        &67.8&         &   &   38.4              \\ \hline
\end{tabular}
}
\caption{\fontsize{10}{12}\selectfont We report F1 on WQSP and CWQ.}\label{tab:wqsp_cwq_path_break}
\end{table}


\begin{table*}[t]\centering
\resizebox{2.1\columnwidth}{!}{%resize the table
\begin{tabular}{l|l|l}
\hline
\multicolumn{3}{c}{Question: what state does romney live in?  \ \ \        Answer: Massachusetts  \ \ \       Topic entity: romney}                                                                                                                                            \\ \hline
.89:children & .29:education\_institution,state\_province\_region & .83:\textbf{places\_lived,location} \\ \hline
.06:\textbf{government\_positions,jurisdiction\_of\_office} & .25:\textbf{places\_lived,location} & .12:\textbf{government\_positions,jurisdiction\_of\_office} \\ \hline
.04:\textbf{government\_positions,office\_position\_or\_title} & .25:\textbf{government\_positions,district\_represented} & .04:\textbf{government\_positions,district\_represented} \\ \hline
.00:\textbf{government\_positions,district\_represented} & .01:\textbf{government\_positions,jurisdiction\_of\_office} & .01:place\_of\_birth,state \\ \hline
.0:place\_of\_birth & .01:place\_of\_birth,state & .00:education,degree \\ \hline
.00:jurisdiction\_of\_office & .01:sibling,place\_of\_birth & .00:election\_campaigns \\ \hline 
\multicolumn{3}{c}{Question:where did madonna grew up?\ \ \       Answer: Bay City\ \ \    Topic entity: madonna}                                                                                                                                            \\ 
\hline
.99:\textbf{place\_of\_birth} & .50:nominated\_for & .47:nominated\_for,featured\_film\_locations \\ \hline
.00:nominated\_for & .16:nominated\_for,featured\_film\_locations & .40:\textbf{place\_of\_birth} \\ \hline
.00:\textbf{place\_lived.location} & .11:compositions & .11:\textbf{place\_lived.location,place} \\ \hline
.00:place\_lived.location,containedby & .09:\textbf{place\_lived.location} & .00:nominated\_for \\ \hline
.00:compositions & .08:\textbf{place\_of\_birth} & .00:\textbf{place\_lived.location} \\ \hline
.00:religion & .01:award\_nominations,nominated\_for & .00:\textbf{place\_lived.location,region} \\ \hline


\end{tabular}
}\caption{\fontsize{10}{12}\selectfont Two running examples from \textsc{WebQuestionSP} dataset. We show the probability $P(\mathbf{r}|q)$ before the inferred relation path. Relation paths that lead to the correct answers are highlighted in bold. The three columns are corresponding to the results by using joint objective with single path, joint objective with multiple paths, and marginal objective with multiple paths. Due to space limit, we only show the partial name of a relation in the example and the probability less than .01 is shown as .00.}\label{tab:case}
\end{table*}

\subsubsection{ComplexWebQuestion-1.1}

This dataset is designed to study complex questions by adding more constraints to questions in \textsc{WebQuestionSP}, so it contains more difficult questions.  Table \ref{tab:wqsp_cwq} shows that our model outperforms all other models in terms of F1. 


\subsection{Further Analysis}

\begin{table}[h]\centering
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{|l|c|c|}
\hline
Setting                          & WQSP  \\
\hline
$\textsc{BEST} - BEST$          & $67.9$ ()      \\
$\textsc{BEST} - f^{entity}$          & $-1.5$ ()      \\
$\textsc{BEST} - ensemble\_pred$          & $-1.3$ ()       \\
$\textsc{BEST}-no inference filter out$          & $-3.4$ ()      \\
$\textsc{BEST} - mutual information$          & $-1.8$ ()      \\
\hline
\end{tabular}
}
\caption{\fontsize{10}{12}\selectfont We report F1 and standard deviation. \textsc{BEST} represents XXX.}\label{tab:wqsp_cwq_ablation}
\end{table}

To further disentangle the contribution of different factor in our proposed method, we present a feature ablation test on WQSP dataset shown in Figure \ref{tab:wqsp_cwq_ablation}. The low standard deviation shows the stable of our proposed method. The vanilla RNN structure only maintain a hidden state and a embedding of the previous prediction at each time-step. Here, we show the performance boost via modeling entity in KBQA task. Instead of using greedy algorithm or beam search to output the top prediction, we propose to predict the set with the highest sum probability, which also improve the performance. We also show the benefit of using inference during training (line XX in algorithm \ref{alg:train}) and mutual information (equation \ref{}). More details can be found in the case study section. 

\subsubsection{PathQuestion-Large}


\begin{table}[h]\centering
\resizebox{1.01\columnwidth}{!}{%resize the table
\begin{tabular}{|l|c|c|c|}
\hline
                            & PQL2H & PQL3H & PQL+  \\
\hline
KV-MemNN \cite{DBLP:conf/emnlp/MillerFDKBW16}    & 72.2 & 67.4 & -         \\
IRN \cite{DBLP:conf/coling/ZhouHZ18}                         & 72.5 & 71.0     & 52.9     \\
HR-BiLSTM \cite{DBLP:conf/acl/YuYHSXZ17}                  & 97.5 & 87.9 & 92.9       \\
ABWIM \cite{DBLP:journals/corr/abs-1801-09893}                      & 94.3 & 89.3 & 92.6         \\
UHop \cite{DBLP:conf/naacl/ChenCCNK19}                       & 97.5 & 89.3 & 92.3      \\
\hline
%Our Method-joint\_prob-nomemory         & 95.3 & 94.8 & 94.5       \\
Our Method-joint\_prob             & 98.3 & 97.1 & 97.5         \\
Our Method-marginal\_prob         & \textbf{98.4} & \textbf{97.8} & \textbf{98.0}       \\
\hline
\end{tabular}
}
\caption{\fontsize{10}{12}\selectfont We report set accuracy ($\%$) on PQL. For UHop, we use the best reported setup from the original paper, \emph{i.e.} ABWIM with UHop.}\label{tab:qpl}
\end{table}

We then test our model on \textsc{PathQuestion-Large} (PQL) dataset.
This dataset contains synthetic questions generated by templates, and is supported by a very small knowledge. In this way, we can see the average performance on this dataset is much stronger than it on the other two datasets. This experiments are mainly used to show the prediction ability on questions with different number of hops. Table \ref{tab:qpl} shows that our method's performance beats all the other approaches on all three subsets of PQL from 1$\%$ to 7.8$\%$ in terms of test accuracy, and especially the gap between our method to the previous state-of-the-art approach (\emph{i.e.} UHop) becomes larger when the number of hops increase from 2 to 3. This indicates that the importance of using our model, especially when the number of hops increases in questions. The high accuracy also indicates that our proposed method can stop at the correct number of hops. 