%\section{Objective Analysis}

 %For each QA pair, the joint objective function pushes the model to allocate all of the probability mass to the given reasoning path. However, this assumption does not reflect real-world reasoning procedures. Figure \ref{QAPaths} shows there could be multiple reasoning paths for one QA sample. \cheng{list issues...cheng!} 
 
 %Although this objective has ability to learn from multiple reasoning paths, it is limited to fit the training data, which means only the labeled reasoning paths are considered. It is not easy to enumerate all reasoning path with human labeling. In addition, this objective has an undesired consequence in practical model training: because of the multiplication operation, the model has to assign equally high probabilities to all given reasoning paths in order to maximize the product of the probabilities. If only some reasoning paths receive high probabilities while others receive low probabilities, the production will still be low. As a consequence, the model cannot differentiate bad reasoning paths from good ones by assigning different probabilities to them. \cheng{list issues...cheng!} 

%\kechen{rephrase} One can easily prove that the above objective pushes the model to assign both high or low probabilities for $P(y|\mathbf{r},q)$ and $P(\mathbf{r}|q)$. It means that good relations always get high probabilities. A benefit of using our model with this objective is that key hashing process filter out irrelevant memory slots from the full search space. During training, the smaller relation paths that point to an answer set, the larger probability mass will be assigned to this relation path. This feature satifies our definition of good path in the introduction. 

%With our proposed training objective, in which the multiplication operation is replaced by the summation operation, it suffices to concentrate only on reasonable reasoning paths for each QA pair. Also, using Jensen's inequality, one can show that this marginal probability objective maximize the answer probability directly which is the learning goal of KBQA task, while joint probability objective maximize a lower bound on the log likelihood.   \cheng{list issues...cheng!} 



\section{Case Study}\label{sec:case}

%\subsection{Threshold Analysis}

Our model requires inference while using the current model to select training samples for next batch in training (see line 6 in Algorithm \ref{alg:train}). This EM style training approach helps us filter out bad relation paths based on context information. For example, a sample question from WQSP is \textit{who was the owner of kfc?}, the graph search algorithm can easily extract two ``correct'' paths starting from the topic entity \textit{kfc} directing to the ground truth answer \textit{Colonel Sanders}: \textit{kfc $\rightarrow$ organization.organization.founders $\rightarrow$ Colonel Sanders} and \textit{kfc $\rightarrow$ advertisingcharacters.product.advertising\_characters $\rightarrow$ Colonel Sanders}. However, the second path is totally wrong given that the relation path is irrelevant to the given question. \textit{Colonel Sanders} happens to be the advertising character of \textit{kfc}, but this cannot be generalized to other cases. Without using the trained model to filter out this irreverent path, the model may learn incorrect map from \textit{who is the owner...} to the relation \textit{advertising\_characters}. In our experiment, we observe that when we train our model with all relation paths generated from DFS algorithm without using this filtering strategy (\emph{i.e.} $k_2=inf$), the F1 score drops $3.4\%$ as shown in Table \ref{tab:wqsp_cwq_ablation}.

Next we demonstrate the benefit of maximizing conditional mutual information instead of likelihood. A sample question in WQSP is \textit{who did benjamin franklin get married to?}. We observe that there are 13 questions are using \textit{Benjamin Franklin} as the topic entity in the training set, but most of them are related to his invention and none of them is about marriage. With such a strong prior on \textit{Benjamin Franklin}, our experimental result shows that the model trained with maximum likelihood maps this question to a path related to \textit{invention}, while the model trained with mutual information make the correct prediction. Table \ref{tab:wqsp_cwq_ablation} shows that we get $1.8\%$ performance boost by using mutual information.




%\subsection{Case Analysis}

We further show how output probabilities look like with different training settings in Table \ref{tab:case}. In the given example, only our method outputs the correct path, and one can also find that the top three results correspond to three different but correct reasoning processes. We observe that in the training data many questions containing phrase \textit{``live in''} contains the topic entity as ``children", which explains why the first model makes wrong prediction. %In the second example, the top one ranked path of our model is not correct. However, with marginalizing out  strategy, it re-discover the correct answer by considering cumulative probabilities of it.
We can see that training with joint objective given a single relation path generates the most sharp relation path distribution, \emph{i.e.} the gap between the top entity and the second one is larger than that using other objectives. It basically only assigns probability to the top relation path. In this case, the model does not have ability to identify multiple relation paths during inference. The other extreme is that the second model is trained with joint objective and multiple input paths, which distribute probabilities over many relation paths, hence the model cannot distinguish good relation paths from the bad ones. Between the above two extremes is the proposed marginal objective with multiple input paths, when the most probable path is assigned with the largest probability, while the rest ones still get reasonable probability assignments. %In this case, it is possible to apply ensemble prediction method to combine the results of beam search outputs and re-rank them. %In our experiment, we observe 1\% performance boost on WQSP in terms of F1 using ensemble prediction method instead of simply selecting the top searching result of beam search as the prediction.

%Another interesting observation is that given the probability of the predicted relation, our model can judge the correctness of the predictions. By filtering out the test samples where the model do not show confidence (\emph{i.e.} probability of the top relation path is less than 0.9), we observe significant improvements on all three models. For example, the F1 score of the third model improves from 0.685 to 0.774 after removing 489 samples on WQSP. It shows the potential to use a second model to further re-rank our prediction results.

